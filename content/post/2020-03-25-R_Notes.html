---
title: "R Notes March 25, 2020"
author: "Elisha Cohen"
output: html_document
header-includes:
    - \usepackage{amsmath}
---




<div id="matrices-in-r" class="section level2">
<h2>Matrices in R!</h2>
<p>We can start by creating a 3X3 matrix and filling it with the numbers 1 to 9:</p>
<pre class="r"><code>A &lt;- matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE)
A</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]    7    8    9</code></pre>
<p>If we use the <code>class()</code> function to check the class it should be of the matrix class:</p>
<pre class="r"><code>class(A)</code></pre>
<pre><code>## [1] &quot;matrix&quot;</code></pre>
<p>This can be important because if matrix operations aren’t working it may be that our object is a data.frame or tibble.</p>
<p>Many of the commands we are used to using on other objects continue to work with matrices. We can add a row or add a column:</p>
<pre class="r"><code>## add a row
A &lt;- rbind(A,c(10,11,12))
A</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]    7    8    9
## [4,]   10   11   12</code></pre>
<pre class="r"><code>## add a column
A &lt;- cbind(A,c(10,20,30,40))
A</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3   10
## [2,]    4    5    6   20
## [3,]    7    8    9   30
## [4,]   10   11   12   40</code></pre>
<p>To access specific elements, rows or columns we use square brackets always specifying row first then column. Currently our <code>A</code> matrix has 4 rows and 4 columns.</p>
<pre class="r"><code># element in the 2nd row, 3rd column
A[2,3]</code></pre>
<pre><code>## [1] 6</code></pre>
<p>To return the <span class="math inline">\(m^{th}\)</span> row:</p>
<pre class="r"><code># return the entire 3rd row
A[3,]</code></pre>
<pre><code>## [1]  7  8  9 30</code></pre>
<p>To return the <span class="math inline">\(n^{th}\)</span> column:</p>
<pre class="r"><code># return the entire 4th column
A[,4]</code></pre>
<pre><code>## [1] 10 20 30 40</code></pre>
<p>If we want to multiply the matrix by a scaler:</p>
<pre class="r"><code>A*2</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    2    4    6   20
## [2,]    8   10   12   40
## [3,]   14   16   18   60
## [4,]   20   22   24   80</code></pre>
<p>Let’s make a 2nd matrix, this time a 4X4 identity matrix:</p>
<pre class="r"><code>B &lt;- diag(c(1,1,1,1))
B</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    1    0    0
## [3,]    0    0    1    0
## [4,]    0    0    0    1</code></pre>
<p>To multiply matrices use <code>%*%</code></p>
<pre class="r"><code>A%*%B</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3   10
## [2,]    4    5    6   20
## [3,]    7    8    9   30
## [4,]   10   11   12   40</code></pre>
<p>Note, that the dimensions must match just like with normal matrix operations. So if we have a matrix that is only 3X3 we will get an error that the arguments are non-comformable.</p>
<pre class="r"><code>C &lt;- matrix(rep(1,8), nrow = 2, ncol = 4)</code></pre>
<pre class="r"><code>A%*%C</code></pre>
<pre><code>## Error in A %*% C: non-conformable arguments</code></pre>
<p>We can transpose a matrix</p>
<pre class="r"><code>C &lt;- t(C)</code></pre>
<p>Now because we have a matrix that is 4X4 and a matrix that is 4X2 our matrix multiplication will work</p>
<pre class="r"><code>A%*%C</code></pre>
<pre><code>##      [,1] [,2]
## [1,]   16   16
## [2,]   35   35
## [3,]   54   54
## [4,]   73   73</code></pre>
<p>To return the inverse of a matrix use solve</p>
<pre class="r"><code>A &lt;- matrix(1:4,nrow=2,ncol=2)
A</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4</code></pre>
<pre class="r"><code>solve(A)</code></pre>
<pre><code>##      [,1] [,2]
## [1,]   -2  1.5
## [2,]    1 -0.5</code></pre>
<p>You can convert a data.frame to a matrix in order to use matrix operations</p>
<pre class="r"><code>mtcars_matrix &lt;- matrix(mtcars)
class(mtcars_matrix)</code></pre>
<pre><code>## [1] &quot;matrix&quot;</code></pre>
</div>
<div id="gapminder" class="section level2">
<h2>Gapminder</h2>
<p>Let’s switch gears and look at the gapminder data.</p>
<pre class="r"><code>library(gapminder)</code></pre>
<p>We will look at the relationship between life expectancy and year controlling for population, and GDP per capita. We expect that life expectancy has increased as time has passed. First, will start with a scatter plot to descriptively look at the relationship. This plots the life expectance of all countries over time.</p>
<pre class="r"><code>library(ggplot2)</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2</code></pre>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.6.2</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>ggplot(data = gapminder, aes(x = year, y = lifeExp)) +
  geom_point() +
  ylab(&#39;Life Expectancy&#39;) +
  xlab(&#39;Year&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-13-1.png" width="672" />
We can add a fitted line</p>
<pre class="r"><code>ggplot(data = gapminder, aes(x = year, y = lifeExp)) +
  geom_point() +
  geom_smooth(method=&#39;loess&#39;, color = &#39;blue&#39;, se=FALSE) +
  ylab(&#39;Life Expectancy&#39;) +
  xlab(&#39;Year&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-14-1.png" width="672" />
It looks like there is a positive relationship between year and life expectancy.</p>
<p>We can also fit a regression line using <code>ggplot2</code></p>
<pre class="r"><code>ggplot(data = gapminder, aes(x = year, y = lifeExp)) +
  geom_point() +
  geom_smooth(method=&#39;lm&#39;, color = &#39;blue&#39;, se=FALSE) +
  ylab(&#39;Life Expectancy&#39;) +
  xlab(&#39;Year&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Now we will fit a linear model of the form:
<span class="math display">\[y = \beta_0 + X \beta + \epsilon\]</span>
where <span class="math inline">\(X\)</span> is a 1706 X 4 matrix because there are 1706 observations (rows) and 4 variables. We can use the built in function <code>lm()</code> to run a linear model with life expectancy as our dependent variable. First, convert the population variable into a logged value.</p>
<pre class="r"><code>gapminder$pop_log &lt;- log(gapminder$pop) # make a logged population variable</code></pre>
<p>Then estimate the linear model
<span class="math display">\[\hat{y} = \hat{\beta}_0 + X\hat{\beta}\]</span></p>
<pre class="r"><code>fit &lt;- lm(lifeExp ~ year + country + pop_log + gdpPercap, data = gapminder)</code></pre>
<p>There are two functions we can use to get the fitted values</p>
<pre class="r"><code>gapminder$yhat &lt;- fit$fitted.values
gapminder$yhat &lt;- fitted.values(fit)</code></pre>
<p>We can check that the difference between our observed <span class="math inline">\(y\)</span> and our <span class="math inline">\(\hat{y}\)</span> is equal to our residuals.</p>
<pre class="r"><code>r &lt;- gapminder$lifeExp - gapminder$yhat
resid &lt;- as.numeric(fit$residuals)
all.equal(r, resid)</code></pre>
<pre><code>## [1] &quot;names for target but not for current&quot;</code></pre>
<p>What is the predicted life expectancy for the United States in 1952 and in 2007?</p>
<pre class="r"><code>US1952 &lt;- gapminder %&gt;% filter(country==&#39;United States&#39; &amp; year==1952)
predict_US1952 &lt;- predict(fit, newdata = US1952)

US2007 &lt;- gapminder %&gt;% filter(country==&#39;United States&#39; &amp; year==2007)
predict_US2007 &lt;- predict(fit, newdata = US2007)
predict_US1952;predict_US2007</code></pre>
<pre><code>##        1 
## 65.42543</code></pre>
<pre><code>##        1 
## 81.42597</code></pre>
<p>We can also extract the variance-covariance matrix from our regression results</p>
<pre class="r"><code>fit_vcov &lt;- vcov(fit)
fit_vcov[1:3,1:3]</code></pre>
<pre><code>##                (Intercept)          year countryAlbania
## (Intercept)    336.2333899 -0.2375184633    13.06769134
## year            -0.2375185  0.0001746932    -0.01129847
## countryAlbania  13.0676913 -0.0112984741     2.73849906</code></pre>
<pre class="r"><code>#extract standard errors
se &lt;- summary(fit)$coefficients[,2] # we want 2nd column
head(se^2,3)</code></pre>
<pre><code>##    (Intercept)           year countryAlbania 
##   3.362334e+02   1.746932e-04   2.738499e+00</code></pre>
<p>The variance-covariance matrix has the variance along the diagonal and we can see that it matches the variance.</p>
</div>
<div id="weighted-least-squares" class="section level2">
<h2>Weighted Least Squares</h2>
<p>We can show that the traditional OLS estimator is unbiased and linear. We are estimating the expectation of <span class="math inline">\(\hat{\beta}\)</span> and assuming that there is constant error variance <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>. This model assumes <span class="math inline">\(\text{Var}(\epsilon_i) = \sigma^2\)</span> for <span class="math inline">\(i = 1, \dots, n\)</span>. In other words, the variance of the residual <span class="math inline">\(\epsilon_i\)</span> for each observation <span class="math inline">\(i\)</span> is equal to <span class="math inline">\(\sigma^2\)</span>.</p>
<p>However, if we think there are nonconstant error variances <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma_{i}^2)\)</span>, we can use Weighted-least squares (WLS) as an alternative estimation method. Here we assume <span class="math inline">\(\text{Var}(\epsilon_i) = \frac{\sigma^2}{w_i}\)</span> where the weights <span class="math inline">\(w_1, \dots w_n\)</span> are known positive constants.</p>
<div id="general-weighted-least-squares-solution" class="section level3">
<h3>General Weighted Least Squares Solution</h3>
<p>Let <span class="math inline">\(W\)</span> be a diagonal matrix with diagonal elements equal to <span class="math inline">\(w_1, \dots w_n\)</span>.</p>
<p><span class="math display">\[
W = \begin{bmatrix} 
    w_{1} &amp; 0 &amp; 0 \\
    0 &amp; \ddots &amp; 0\\
    0 &amp;   0    &amp; w_{n} 
    \end{bmatrix}
\]</span>
Weighted residual sum of squares
<span class="math display">\[S_w(\beta) = \sum_{i=1}^n w_i(y_i - x_{i}^t \beta)^2 = (Y - X\beta)^t W (Y-X\beta)\]</span>
WLS estimates <span class="math inline">\(\beta\)</span> by minimizing the weighted sum of squares. The general solution is:
<span class="math display">\[\hat{\beta} = (X^t W X)^{-1}X^t WY\]</span></p>
</div>
<div id="the-weights" class="section level3">
<h3>The weights</h3>
<p>To use WLS we need to know the weights.</p>
<ol style="list-style-type: decimal">
<li>We may have a probabilistic model in mind for the weights where <span class="math inline">\(\text{Var}(Y|X=x_i)\)</span>. For example we may think that the errors are proportional to a specific explanatory variable <span class="math inline">\(x_i\)</span>. Then we could use the reciprocal of this to estimate the weights: <span class="math inline">\(w_i = 1/x_i\)</span>.</li>
</ol>
<p>For example let’s simulate data from a Poisson distribution.</p>
<pre class="r"><code>set.seed(12345)
x &lt;- rpois(100, lambda = 2) 
y &lt;- 3 + 4*x + rnorm(100, sd=2*x)
ggplot(data = data.frame(x,y),aes(x=x,y=y)) +
  geom_point() +
  geom_smooth(method=lm,se=FALSE) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Notice how the variance in <span class="math inline">\(y\)</span> gets larger as <span class="math inline">\(x\)</span> gets larger. Now if we estimate a non-weighted linear regression:</p>
<pre class="r"><code>fit_lm &lt;- lm(y ~ x)
summary(fit_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.4022  -2.9553  -0.7186   2.5219  18.7514 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.7186     1.1052   3.365   0.0011 ** 
## x             4.0867     0.4305   9.492 1.53e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.417 on 98 degrees of freedom
## Multiple R-squared:  0.479,  Adjusted R-squared:  0.4737 
## F-statistic: 90.11 on 1 and 98 DF,  p-value: 1.53e-15</code></pre>
<p>Our <span class="math inline">\(\hat{\beta}\)</span> estimate is 4.087 which is close to 4.</p>
<p>Now let’s estimate using WLS:</p>
<pre class="r"><code>w &lt;- 1/x #note there are 100 weights, one for each observation
w[w==&#39;Inf&#39;] &lt;- 0 #fix any weights that occurred because 1/0 doesn&#39;t exist
fit_wls &lt;- lm(y ~ x, weights = w)
summary(fit_wls)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, weights = w)
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.492  -2.343   0.000   1.756   9.080 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.1001     1.0529   2.944  0.00419 ** 
## x             4.3894     0.5065   8.666 2.79e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.793 on 84 degrees of freedom
## Multiple R-squared:  0.472,  Adjusted R-squared:  0.4657 
## F-statistic:  75.1 on 1 and 84 DF,  p-value: 2.791e-13</code></pre>
<p>Again, 4.389 is close to the true value of 4. If we look at a plot we can see that the weighted line in blue is above the red line as <span class="math inline">\(x\)</span> gets larger. This is because the variance is getting larger and the WLS is downweighting these observations.</p>
<pre class="r"><code>ggplot(data = data.frame(x,y),aes(x=x,y=y)) +
  geom_point() +
  geom_smooth(method=lm,se=FALSE,color=&#39;red&#39;) +
  geom_smooth(method=lm,aes(weight=w),color=&#39;blue&#39;,se=FALSE) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Usually we do not know the weights nor the structure of <span class="math inline">\(W\)</span> so we will first estimate the OLS regression.</li>
</ol>
<p>Simulate data with non-constant errors:</p>
<pre class="r"><code>set.seed(12345)
scalar &lt;- 2
x &lt;- rep(1:100,2)
sigma2 &lt;- x^scalar
eps &lt;- rnorm(x,mean=0,sd=sqrt(sigma2))
beta0 &lt;- 3
beta1 &lt;- 1
y &lt;- beta0+beta1*x + eps
# fit linear model
fit_lm &lt;- lm(y ~ x)
resid &lt;- summary(fit_lm)$residuals
yhat &lt;- fit_lm$fitted.values
# plot the residuals compared to x variable
ggplot(data.frame(x,resid),aes(x = x, y = resid)) +
  geom_point() +
  geom_hline(yintercept=0,color=&#39;green&#39;,linetype=&#39;dashed&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Looking at the plot of residuals compared to x values we see that as x gets larger so do the residuals.</p>
<p>There are different ways to calculate the weights using residuals. Here we are going to do the following steps:</p>
<ul>
<li>regress the residuals on the fitted values from the OLS model</li>
<li>extract the fitted values from this regression</li>
<li>use these fitted values squared as estimates of <span class="math inline">\(\sigma_{i}^2\)</span> therefore we can estimate the weights as 1 over the square of the fitted values (<span class="math inline">\(w_i = 1/\sigma_{i}^2\)</span>)</li>
</ul>
<pre class="r"><code># regress residuals on fitted values
fit_resid_yhat &lt;- lm(abs(resid) ~ yhat)
# take fitted value of these
fitted &lt;- fit_resid_yhat$fitted.values
w &lt;- 1/(fitted^2)
fit_wls &lt;- lm(y ~ x, weights = w)</code></pre>
<pre class="r"><code>htmlreg(list(fit_lm,fit_wls),custom.model.names = c(&quot;OLS&quot;,&quot;WLS&quot;),doctype = F)</code></pre>
<table class="texreg" style="margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;">
<caption>
Statistical models
</caption>
<thead>
<tr>
<th style="padding-left: 5px;padding-right: 5px;">
 
</th>
<th style="padding-left: 5px;padding-right: 5px;">
OLS
</th>
<th style="padding-left: 5px;padding-right: 5px;">
WLS
</th>
</tr>
</thead>
<tbody>
<tr style="border-top: 1px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
(Intercept)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-2.60
</td>
<td style="padding-left: 5px;padding-right: 5px;">
1.82
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(8.50)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(2.26)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
x
</td>
<td style="padding-left: 5px;padding-right: 5px;">
1.30<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
1.20<sup>***</sup>
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.15)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.10)
</td>
</tr>
<tr style="border-top: 1px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
R<sup>2</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.29
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.44
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Adj. R<sup>2</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.28
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.43
</td>
</tr>
<tr style="border-bottom: 2px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
Num. obs.
</td>
<td style="padding-left: 5px;padding-right: 5px;">
200
</td>
<td style="padding-left: 5px;padding-right: 5px;">
200
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="font-size: 0.8em;" colspan="3">
<sup>***</sup>p &lt; 0.001; <sup>**</sup>p &lt; 0.01; <sup>*</sup>p &lt; 0.05
</td>
</tr>
</tfoot>
</table>
<p>Both models estimate <span class="math inline">\(\beta_1\)</span> pretty well. However, we’ve simulated the data with intercept equal to 3 but we see that the OLS estimate of the intercept is -2.6 while the WLS estimate of the intercept is closer at 1.82. Also note the big decrease in root mean squared error (RMSE) in the WLS model.</p>
<p>Plot both:</p>
<pre class="r"><code>ggplot(data = data.frame(x,y),aes(x=x,y=y)) +
  geom_point() +
  geom_smooth(method=lm,se=FALSE,color=&#39;red&#39;) +
  geom_smooth(method=lm,aes(weight=w),color=&#39;blue&#39;,se=FALSE) +
  theme_bw()</code></pre>
<p><img src="/post/2020-03-25-R_Notes_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Again, we see that the WLS shifts slightly compared to OLS in order to down weight observations with extreme variances.</p>
<p>Note, that there are different ways to estimate the residuals. We can do it as in part 1 as the reciprocal of a specific predictor. Alternatively, the variances may be close to constant within specific subsets of a predictor. Or we might first estimate the OLS model and then use the residuals to help us estimate weights.</p>
</div>
</div>
